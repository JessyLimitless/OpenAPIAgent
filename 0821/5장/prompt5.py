import streamlit as st
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# OpenAI API í‚¤
OPENAI_API_KEY = ''

# Streamlit í™”ë©´ êµ¬ì„±
st.title("ğŸ“„ í…ìŠ¤íŠ¸ ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")
st.write("ì—…ë¡œë“œí•œ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.")

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt"])

if uploaded_file is not None:
    # íŒŒì¼ ë‚´ìš© ì½ê¸°
    document = uploaded_file.read().decode("utf-8")
    
    # ë¬¸ì„œ ë²¡í„°í™” ë° ì„ë² ë”© ì²˜ë¦¬
    documents = [{"text": document}]
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    
    # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vector_store = FAISS.from_texts([doc["text"] for doc in documents], embeddings)
    
    # OpenAI GPT ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-3.5-turbo")
    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever(), chain_type="stuff")
    
    st.write("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")
    
    # ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¶œë ¥
    if question:
        answer = qa_chain({"query": question})
        st.write("### ì§ˆë¬¸:")
        st.write(question)
        st.write("### ë‹µë³€:")
        st.write(answer['result'])

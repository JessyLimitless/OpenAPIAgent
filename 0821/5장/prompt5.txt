import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = '' # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_text(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    return text # í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ì…ë ¥
file_path = input("í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
document = load_text(file_path)  # ë¬¸ì„œë¥¼ Langchain í˜•ì‹ìœ¼ë¡œ ë¡œë“œ
documents = [{"text": document}]

# ë¬¸ì„œ ì„ë² ë”© ìƒì„±
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

# Faiss ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
vector_store = FAISS.from_texts([doc["text"] for doc in documents], embeddings)

# ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì²´ì¸ ìƒì„±
llm = ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-3.5-turbo")
qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever(), chain_type="stuff")

print("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.") # ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬
while True:
    question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤): ")
    if question.lower() == "ì¢…ë£Œ":
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹")
        break
    if question:
        # ì§ˆë¬¸ì— ë‹µë³€
        answer = qa_chain({"query": question})
        print("ì§ˆë¬¸:", question)
        print("ë‹µë³€:", answer['result'])
    else:
        print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.") 
        
        ìœ„ ì½”ë“œëŠ” ì£¼í”¼í„°ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰ë˜ëŠ” í…ìŠ¤íŠ¸ ë¬¸ì„œ ê¸°ë°˜í˜• ì±—ë´‡ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ìŠ¤íŠ¸ë¦¼ë¦¿ì„ ì‚¬ìš©í•´ì„œ í™”ë©´ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

OPENAI_API_KEY = ''

def load_text(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    return text

file_path = input("í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
document = load_text(file_path)

documents = [{"text": document}]

embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

vector_store = FAISS.from_texts([doc["text"] for doc in documents], embeddings)

llm = ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-3.5-turbo")
qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever(), chain_type="stuff")
print("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

while True:
    question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤): ")
    if question.lower() == "ì¢…ë£Œ":
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹")
        break
    if question:
        # ì§ˆë¬¸ì— ë‹µë³€
        answer = qa_chain({"query": question})
        print("ì§ˆë¬¸:", question)
        print("ë‹µë³€:", answer['result'])
    else:
    
        print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

ìœ„ ì½”ë“œëŠ” ì£¼í”¼í„°ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰ë˜ëŠ” í…ìŠ¤íŠ¸ ë¬¸ì„œ ê¸°ë°˜í˜• ì±—ë´‡ì…ë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•´ ì£¼ì„¸ìš”. ì—¬ê¸°ì— ìŠ¤íŠ¸ë¦¼ë¦¿ì„ ì‚¬ìš©í•´ì„œ í™”ë©´ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.


